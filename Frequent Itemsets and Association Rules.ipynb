{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors: Oguzhan Ugur, Shadman Ahmed\n",
    "## Discovery of Frequent Itemsets and Association Rules\n",
    "\n",
    "\n",
    "### Problem Formulation\n",
    "Frequent itemsets is about finding sets of items that appear in number of baskets at the same time. This model is called \"market-basket\" model of data, where the realtionship between the \"items\" and \"baskets\" are important. <br>\n",
    "<br>\n",
    "In this homework, we were instructed to implement the apriori algorithm for finding frequent itemsets, with support at least s. We were also instructed to generate association rules between the frequent itemsets with at least confidence c and support s in a data set of sales transaction. \n",
    "\n",
    "### Dataset\n",
    "The data used in this homework was a sale transaction dataset which included generated transactions(baskets) of hashed  items. \n",
    "\n",
    "### Apriori Algorithm\n",
    "The apriori algorithm finds the frequent itemsets of the dataset by taking a minimum support value as input. This algorithm will generate a dictionary with the frequent itemsets as key and the index of baskets the items appear in as value of the dictionary. This dictionary will then be scanned in order to filter out the items with less support than s. The remaining items are then combined to create doubletons(itemsets with two elements). The doubletons will then be scanned and the doubletons with less support than s will be filtered out again. We chose to repeat this procedure to a chosen number of combinations k. So, it is possible to choose the size of the itemsets. \n",
    "\n",
    "#### Code\n",
    "The apriori algorithm consist of 4 functions, <br>\n",
    "- First, \"all_items_list_of_occurances\" that takes the dataset file as input and returns a dictionary. The key values of the dictionary will be the itemsets and the value are a list of corresponding baskets that the itemset appears in. \n",
    "- Second, \"Scanner\" that filters out the itemsets with less support than a chosen number s and returns a dictionary with the same structure as before. \n",
    "- Third, \"combination\" that creates the combinations between the filtered itemsets and returns a set of these combinations. \n",
    "- Fourth, \"scan_for_combinations\" that scans the set of combinations created on the previous function and returns a dictionary with the combined itemsets(new itemsets) as key and the value of this dictionary are the corresponding baskets that these combined items appear in. \n",
    "\n",
    "The Second,Third and Fourth points are repeated in order to create more combinations of these itemssets and see the most frequent items that appear together.\n",
    "\n",
    "### Notice \n",
    "The Association rule is described below after the code of the Apriori Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set variables\n",
    "s = 2          #Support threshold\n",
    "k = 3            #Maximum combinations\n",
    "c = 0.8          #Confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_items_list_of_occurances(file):\n",
    "    \n",
    "    all_items = defaultdict(set)\n",
    "\n",
    "    with open(file, \"r\") as f:\n",
    "        for trans_index,trans in enumerate(f):\n",
    "            trans_clean = trans.split()\n",
    "\n",
    "            for item in trans_clean:\n",
    "                    all_items[frozenset([item])].add(trans_index)\n",
    "        return(all_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scanner(all_items, s):\n",
    "\n",
    "    C_k_scanned = {}\n",
    "    rest = {}\n",
    "    \n",
    "    for item, occurance_list in all_items.items():\n",
    "        if(len(occurance_list) >= s):\n",
    "            C_k_scanned[item] = occurance_list\n",
    "\n",
    "\n",
    "    return(C_k_scanned)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination(k_tons, single_tons,k):\n",
    "    combination = {items.union(item) for items in k_tons.keys()\n",
    "                                        for item in single_tons.keys()\n",
    "                                          if len(items.union(item)) == k}\n",
    "    \n",
    "    return combination\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_for_combinations(combinations, singelton_dict):\n",
    "\n",
    "    scaned_ck = {}\n",
    "    for combination in combinations:\n",
    "        all_occurances = (singelton_dict[frozenset([item])] for item in combination)\n",
    "        occurances = set.intersection(*all_occurances)\n",
    "        \n",
    "        scaned_ck[combination] = occurances\n",
    "    \n",
    "    return(scaned_ck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{frozenset({'704', '825'}): {1, 3}, frozenset({'825', '834'}): {0, 1}, frozenset({'39', '834'}): {1}, frozenset({'704', '834'}): {1}, frozenset({'704', '39'}): {1, 3}, frozenset({'825', '39'}): {1, 3}}\n",
      "----------\n",
      "{frozenset({'825', '39', '834'}): {1}, frozenset({'704', '39', '834'}): {1}, frozenset({'704', '825', '834'}): {1}, frozenset({'704', '825', '39'}): {1, 3}}\n",
      "----------\n",
      "{frozenset({'704', '825', '39'}): {1, 3}}\n"
     ]
    }
   ],
   "source": [
    "all_items = all_items_list_of_occurances(\"T10I4D5.dat\")\n",
    "\n",
    "L1 = Scanner(all_items, s)\n",
    "\n",
    "C2 = combination(L1,L1,2)\n",
    "\n",
    "C2 = scan_for_combinations(C2, L1)\n",
    "print(C2)\n",
    "print(\"----------\")\n",
    "L2 = Scanner(C2, s)\n",
    "\n",
    "C3 = combination(L2,L1,3)\n",
    "\n",
    "C3 = scan_for_combinations(C3, L1)\n",
    "print(C3)\n",
    "print(\"----------\")\n",
    "L3 = Scanner(C3, s)\n",
    "\n",
    "print(L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(s,k): \n",
    "    all_items = all_items_list_of_occurances(\"T10I4D5.dat\")\n",
    "    singleton_list= Scanner(all_items, s)\n",
    "    kton_supported=singleton_list\n",
    "    \n",
    "    k_dict = {}\n",
    "    k_dict[1] = singleton_list\n",
    "    for k in range(2,k+1):\n",
    "        k_comb = combination(kton_supported,singleton_list,k)\n",
    "        kton_value = scan_for_combinations(k_comb, singleton_list)\n",
    "        kton_supported= Scanner(kton_value,s)\n",
    "    \n",
    "        k_dict[k] = kton_supported\n",
    "    return(kton_supported,k_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_items_dict, k_dict= main(s,k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items                                             support   \n",
      "frozenset({'825'}):                               3         \n",
      "frozenset({'834'}):                               2         \n",
      "frozenset({'39'}):                                2         \n",
      "frozenset({'704'}):                               2         \n",
      "frozenset({'704', '825'}):                        2         \n",
      "frozenset({'825', '834'}):                        2         \n",
      "frozenset({'704', '39'}):                         2         \n",
      "frozenset({'825', '39'}):                         2         \n",
      "frozenset({'704', '825', '39'}):                  2         \n"
     ]
    }
   ],
   "source": [
    "#print frequent itemset with at least support s \n",
    "\n",
    "format = \"{:<50}{:<10}\"  \n",
    "print(format.format(\"items\" , \"support\"))\n",
    "for item_sets in range(1,len(k_dict)+1):\n",
    "    for items in k_dict[item_sets]:\n",
    "        print(format.format(str(items)+ \": \" \n",
    "                                   , str(len(k_dict[item_sets][items]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The association rules algorithm generates rules about the contents of the transactions. Namely, if a transaction contains a collection of items then it is also likely to contain a particular item $j$. An association rule can be stated as following: {$i_1$, $i_2$ , , $i_k$}  $\\rightarrow$ $j$ if the transaction contains items {$i_1$, $i_2$ , , $i_k$} then it is likely to also contain $j$. The confidence of this association rule is the fraction of {$i_1$, $i_2$ , , $i_k$, $j$} support number and the {$i_1$, $i_2$ , , $i_k$} support number.\n",
    " \n",
    " \n",
    "The association rules algorithm consist of three functions,<br>\n",
    "- First, The \"create_frequent_item_list\" function that returns a list of the frequent itemset produced by the apriori algorithm.\n",
    "\n",
    "- Second, The \"generate_association\" function that takes the frequent itemset list and extracts two itemsets at a time, item_set1 and item_set2. If item_set1 is a subset of item_set2 then we create an association rule where we calculate the confidence score by using the function \"confidence\". Then we print out the association rules with confidence above $c$ threshold.\n",
    "\n",
    "- Third, The \"confidence\" function takes item_set2 and extract the key from the frequent itemset list (which is the corresponding support number). And divide it by the support number of item_set1, which is a subset of item_set2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequent_item_list(k_dict):\n",
    "    L = []\n",
    "    for k1 in range(1,len(k_dict)+1):\n",
    "        for items in k_dict[k1]:\n",
    "            L.append(items)\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_association(k_dict, conf_threshold = 0.7):\n",
    "    \n",
    "    L = create_frequent_item_list(k_dict)\n",
    "    result = []\n",
    "    \n",
    "    format = \"{:<30}{:<10}{:<40}{:<10}\"    \n",
    "    print(format.format(\"confidence denominator\",\" \",\"numerator = denom + item_below\", \"confidence score\"))\n",
    "    for i in range(len(L)):\n",
    "        for j in range(i+1,len(L)):\n",
    "            if set(L[i]).issubset(L[j]):\n",
    "                conf = confidence(k_dict,L[i],L[j])\n",
    "                if(conf >= conf_threshold):\n",
    "                    conf = (\"%.2f\" % conf)\n",
    "                    print(format.format(str(L[i]),\"=>\",str(L[j].difference(L[i])),conf))\n",
    "                    associations = (str(L[i])+\" => \"+str(L[j])+\" with confidence: \"+ str(conf))\n",
    "                    result.append(associations)\n",
    "                    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(frequent_item_set,item_set1,item_set2):\n",
    "    \n",
    "    union = len(frequent_item_set[len(item_set2)][item_set2])\n",
    "    sup_itemset = len(frequent_item_set[len(item_set1)][item_set1])\n",
    "    \n",
    "    conf = union/sup_itemset\n",
    "    \n",
    "    return conf   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence denominator                  numerator = denom + item_below          confidence score\n",
      "frozenset({'834'})            =>        frozenset({'825'})                      1.00      \n",
      "frozenset({'39'})             =>        frozenset({'704'})                      1.00      \n",
      "frozenset({'39'})             =>        frozenset({'825'})                      1.00      \n",
      "frozenset({'39'})             =>        frozenset({'704', '825'})               1.00      \n",
      "frozenset({'704'})            =>        frozenset({'825'})                      1.00      \n",
      "frozenset({'704'})            =>        frozenset({'39'})                       1.00      \n",
      "frozenset({'704'})            =>        frozenset({'825', '39'})                1.00      \n",
      "frozenset({'704', '825'})     =>        frozenset({'39'})                       1.00      \n",
      "frozenset({'704', '39'})      =>        frozenset({'825'})                      1.00      \n",
      "frozenset({'825', '39'})      =>        frozenset({'704'})                      1.00      \n"
     ]
    }
   ],
   "source": [
    "result = generate_association(k_dict, c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
